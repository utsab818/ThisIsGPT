{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSONsb28bAJF"
      },
      "source": [
        "# Multi-Head attention with weight splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "9u4fLHoVcNUX"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "    super().__init__()\n",
        "\n",
        "    assert (d_out % num_heads) == 0, \"d_out must be divisible by num_heads\"\n",
        "\n",
        "    self.d_out = d_out\n",
        "    self.num_heads = num_heads\n",
        "    # calculate individual head dimension according to d_out and no. of heads present\n",
        "    self.head_dim = d_out // num_heads\n",
        "\n",
        "    # random key,query,value initialization with d_in and d_out)\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.out_proj = nn.Linear(d_in, d_out) # Linear layer to combine head outputs\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    self.register_buffer(\n",
        "        \"mask\",\n",
        "        torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    b, num_tokens, d_in = x.shape # initialize (Batch, token_size, input_dimension)\n",
        "\n",
        "    # keys, values queries (random of d_in,d_out(dimensions) multiplied with inputs)\n",
        "    keys = self.W_key(x)\n",
        "    queries = self.W_query(x)\n",
        "    values = self.W_value(x)\n",
        "\n",
        "    # convert of each head i.e d_out --> num_heads and head_dimension\n",
        "    keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "    queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "    values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "    # Group matrices by num_heads for parallel computation.\n",
        "\n",
        "    #(b,num_tokens,num_heads,head_dim) --> (b, num_heads, num_tokens, head_dim)\n",
        "    # (1,3,2,3) --> (1,2,3,3) (The positions 1 and 2 will be transposed)\n",
        "    keys = keys.transpose(1,2)\n",
        "    queries = queries.transpose(1,2)\n",
        "    values = values.transpose(1,2)\n",
        "\n",
        "    # now for each query we will do matmul with keys.\n",
        "    # and for that we need to transpose the postion 2 and 3 of keys.\n",
        "    # (b,num_heads,num_tokens,head_dim) * (b, num_heads, head_dim, num_tokens)\n",
        "    #                                    |\n",
        "    #                    (b,num_heads,num_tokens,num_tokens)\n",
        "    attn_scores = queries @ keys.transpose(2,3)\n",
        "\n",
        "    # masking\n",
        "    mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "    attn_scores = attn_scores.masked_fill(mask_bool, -torch.inf)\n",
        "\n",
        "    # softmax with Sqrt of head_dim and dropout\n",
        "    attn_weights = torch.softmax(attn_scores / self.head_dim**0.5, dim=-1)\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "    # calulate context vector with d_out as dimension preserved\n",
        "\n",
        "    # (b,num_heads,num_tokens,num_tokens) * (b,num_heads,num_tokens,head_dim)\n",
        "    #                                     |\n",
        "    #                     (b,num_heads,num_tokens,head_dim)\n",
        "    #                                     | (1,2) transpose\n",
        "    #                     (b,num_tokens,num_heads,head_dim)\n",
        "    context_vector = (attn_weights @ values).transpose(1,2)\n",
        "    # now we can merge num_heads and head_dim easily to d_out.\n",
        "    # we merge the num_heads and head_dim into single row giving d_out dimension.\n",
        "    # (b,num_tokens,num_heads,head_dim) --> (b,num_tokens,d_out)\n",
        "    # contiguous ensures that after reshaping the values stay in same block of memory.\n",
        "    context_vector = context_vector.contiguous().view(b, num_tokens, self.d_out)\n",
        "    context_vector = self.out_proj(context_vector)\n",
        "\n",
        "    return context_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URMPxF6jwMIE",
        "outputId": "ae3a3707-b1dd-4f1c-b91d-b9cc3093b2ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 3, 6])\n",
            "tensor([[[ 0.1569, -0.0873,  0.0210,  0.0215, -0.3243, -0.2518],\n",
            "         [ 0.1117, -0.0547,  0.0406, -0.0213, -0.3251, -0.2993],\n",
            "         [ 0.1196, -0.0491,  0.0318, -0.0635, -0.2788, -0.2578]],\n",
            "\n",
            "        [[ 0.1569, -0.0873,  0.0210,  0.0215, -0.3243, -0.2518],\n",
            "         [ 0.1117, -0.0547,  0.0406, -0.0213, -0.3251, -0.2993],\n",
            "         [ 0.1196, -0.0491,  0.0318, -0.0635, -0.2788, -0.2578]]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "Context vector shape:  torch.Size([2, 3, 6])\n"
          ]
        }
      ],
      "source": [
        "# example test\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# define input of 3 row and 6 columns\n",
        "inputs = torch.tensor(\n",
        "    [[0.43,0.15,0.89,0.55,0.87,0.66],\n",
        "     [0.57,0.85,0.64,0.22,0.58,0.33],\n",
        "     [0.77,0.25,0.10,0.05,0.80,0.55]]\n",
        ")\n",
        "\n",
        "# copy same input and stack on top for 2 batch\n",
        "batch = torch.stack((inputs,inputs), dim=0)\n",
        "print(batch.shape)\n",
        "\n",
        "batch_size, context_length, d_in = batch.shape\n",
        "d_out = 6\n",
        "\n",
        "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
        "context_vec = mha(batch)\n",
        "\n",
        "print(context_vec)\n",
        "print(\"Context vector shape: \", context_vec.shape)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
