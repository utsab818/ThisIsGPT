{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip3 install tiktoken\n",
        "! pip3 install striprtf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKGY5Enmq2Wy",
        "outputId": "39f7ddcb-f1b7-4338-eb7d-1da526aa78a5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2026.1.4)\n",
            "Requirement already satisfied: striprtf in /usr/local/lib/python3.12/dist-packages (0.0.29)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from striprtf.striprtf import rtf_to_text\n",
        "\n",
        "with open(\"/text.rtf\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "raw_text = rtf_to_text(raw_text)\n",
        "\n",
        "print(\"Total number of characters (clean): \", len(raw_text))\n",
        "print(\"--- Preview ---\")\n",
        "print(raw_text[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToWW4s-3zKQ_",
        "outputId": "df4cc0db-3bf9-4426-a399-fdb675b493d4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of characters (clean):  2982\n",
            "--- Preview ---\n",
            "Personal Statement\n",
            "Engineering the best solutions to complex problems is rarely a matter of raw com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import tiktoken\n",
        "print(\"Tiktoken version: \", importlib.metadata.version('tiktoken'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ifA7ZyNq8B_",
        "outputId": "19a9a2a7-6f3d-415b-9815-2bf2f3816a39"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tiktoken version:  0.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "hWikGuiyrDT7"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc_text = tokenizer.encode(raw_text)\n",
        "print(len(enc_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEVqW-FprWp7",
        "outputId": "45a7e2a0-5876-48f8-f193-3f93b3588f50"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "639\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_size = 4"
      ],
      "metadata": {
        "id": "4VK84ILAsdyK"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = enc_text[:context_size]\n",
        "y = enc_text[1:context_size+1]\n",
        "\n",
        "print(f\"x: {x}\")\n",
        "print(f\"y:        {y}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjjEi-Ot1xq-",
        "outputId": "17152822-a8bc-40e5-cb4b-202825e80055"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: [30228, 21983, 198, 13798]\n",
            "y:        [21983, 198, 13798, 1586]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, context_size+1):\n",
        "  context = enc_text[:i]\n",
        "  target = enc_text[i]\n",
        "\n",
        "  print(context, \"---->\", target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFeazrAJ2sdd",
        "outputId": "b250cefb-dbb6-4f8d-88b0-2dc9b93984bc"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[30228] ----> 21983\n",
            "[30228, 21983] ----> 198\n",
            "[30228, 21983, 198] ----> 13798\n",
            "[30228, 21983, 198, 13798] ----> 1586\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, context_size+1):\n",
        "  context = enc_text[:i]\n",
        "  target = enc_text[i]\n",
        "\n",
        "  print(tokenizer.decode(context), \"---->\", tokenizer.decode([target]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQoKdq6m3rsU",
        "outputId": "b77d68cc-cdc3-4049-d03c-5927af879fee"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Personal ---->  Statement\n",
            "Personal Statement ----> \n",
            "\n",
            "Personal Statement\n",
            " ----> Engine\n",
            "Personal Statement\n",
            "Engine ----> ering\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Implementing a Data Loader**"
      ],
      "metadata": {
        "id": "S4ys_KMW6bGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "GD_rav5C6pLl"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTDatasetV1(Dataset):\n",
        "  def __init__(self, txt, tokenizer, max_length, stride):\n",
        "    self.input_ids = []\n",
        "    self.target_ids = []\n",
        "\n",
        "    # tokenize the entire text\n",
        "    token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "    # use sliding window with max_length as sequence length\n",
        "    # since we will be sliding by 4 words, the stride here will be 4\n",
        "    # which means we skip 4 words to create next sequence.\n",
        "    for i in range(0, len(token_ids)-max_length, stride):\n",
        "      input_chunk = token_ids[i:i+max_length]\n",
        "      target_chunk = token_ids[i+1:i+max_length+1]\n",
        "      self.input_ids.append(torch.tensor(input_chunk))\n",
        "      self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "MX4taBk2FzN5"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True,\n",
        "                         num_workers=0):\n",
        "\n",
        "  # Initialize the tokenizer\n",
        "  tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "  # Initialize the dataset\n",
        "  dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "  # Create the dataloader\n",
        "  dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle,\n",
        "                          drop_last=drop_last, num_workers=num_workers)\n",
        "\n",
        "  return dataloader"
      ],
      "metadata": {
        "id": "mDb_snY3Jv_M"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# convert the dataloader to python iterator to get the data batch from raw_text\n",
        "dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4,\n",
        "                                  stride=1, shuffle=False)\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "\n",
        "print(first_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34P7q8pzL0qz",
        "outputId": "2a1bb219-398d-4da0-8045-93bd1f7a8d9e"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[30228, 21983,   198, 13798]]), tensor([[21983,   198, 13798,  1586]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "second_batch = next(data_iter)\n",
        "print(second_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcFN3HaANMiv",
        "outputId": "0f497cee-f20f-467a-f64d-15043d3acea8"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[21983,   198, 13798,  1586]]), tensor([[  198, 13798,  1586,   262]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# experiment with batch_size and stride\n",
        "dataloader = create_dataloader_v1(raw_text, batch_size=8,\n",
        "                                  max_length=4, stride=4,\n",
        "                                  shuffle=False)\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "\n",
        "print(\"Inputs:\\n\", inputs)\n",
        "print(\"\\nTargets:\\n\",  targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6pzMig8N7t_",
        "outputId": "468ef4ce-d415-43b0-b80d-69e9ac227ad1"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs:\n",
            " tensor([[30228, 21983,   198, 13798],\n",
            "        [ 1586,   262,  1266,  8136],\n",
            "        [  284,  3716,  2761,   318],\n",
            "        [ 8365,   257,  2300,   286],\n",
            "        [ 8246, 14492,  1176,   198],\n",
            "        [17749,    26,   340,   318],\n",
            "        [ 8793,   416,  8263,   422],\n",
            "        [  262, 10084, 45207,   286]])\n",
            "\n",
            "Targets:\n",
            " tensor([[21983,   198, 13798,  1586],\n",
            "        [  262,  1266,  8136,   284],\n",
            "        [ 3716,  2761,   318,  8365],\n",
            "        [  257,  2300,   286,  8246],\n",
            "        [14492,  1176,   198, 17749],\n",
            "        [   26,   340,   318,  8793],\n",
            "        [  416,  8263,   422,   262],\n",
            "        [10084, 45207,   286,   661]])\n"
          ]
        }
      ]
    }
  ]
}