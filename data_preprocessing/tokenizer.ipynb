{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQ7IsnpvdkmV",
        "outputId": "251f4269-63b3-491f-8b97-a3f0ea166df2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: striprtf in /usr/local/lib/python3.12/dist-packages (0.0.29)\n"
          ]
        }
      ],
      "source": [
        "!pip install striprtf\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxHEbLUicdpM",
        "outputId": "79d2f64b-93cb-49fd-d801-50654be730b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of characters (clean):  1791\n",
            "--- Preview ---\n",
            "Research Statement\n",
            "I want to understand the true limits of what neural networks can compute exactly\n"
          ]
        }
      ],
      "source": [
        "from striprtf.striprtf import rtf_to_text\n",
        "\n",
        "with open(\"/text.rtf\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "raw_text = rtf_to_text(raw_text)\n",
        "\n",
        "print(\"Total number of characters (clean): \", len(raw_text))\n",
        "print(\"--- Preview ---\")\n",
        "print(raw_text[:99])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHY6CoFpf8ej",
        "outputId": "c9924828-3b32-4207-e688-b57486fd485e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Research', 'Statement', 'I', 'want', 'to', 'understand', 'the', 'true', 'limits', 'of', 'what', 'neural', 'networks', 'can', 'compute', 'exactly', '.', 'A', 'recent', 'paper', 'from', 'the', 'University', 'of', 'Waterloo', 'titled', '“Learning', 'to', 'Add', ',']\n"
          ]
        }
      ],
      "source": [
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(preprocessed[:30])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydPdc61ghGZh",
        "outputId": "841755b9-5cf2-4f3c-8737-86cf42939d4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "181\n"
          ]
        }
      ],
      "source": [
        "all_words = sorted(set(preprocessed))\n",
        "vocab_size = len(all_words)\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "21AAeLF5io6r"
      },
      "outputs": [],
      "source": [
        "vocab = {token:integer for integer,token in enumerate(all_words)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l03tSYJdi9LS",
        "outputId": "3ef50fe2-1b94-4d74-c035-044939616e7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(',', 0)\n",
            "('.', 1)\n",
            "('?', 2)\n",
            "('A', 3)\n",
            "('AI', 4)\n",
            "('Add', 5)\n",
            "('Algorithmic', 6)\n",
            "('Can', 7)\n",
            "('Exactly', 8)\n",
            "('Execute', 9)\n",
            "('I', 10)\n",
            "('If', 11)\n",
            "('Instructions', 12)\n",
            "('I’ll', 13)\n",
            "('Multiply', 14)\n",
            "('My', 15)\n",
            "('Networks”', 16)\n",
            "('Neural', 17)\n",
            "('Research', 18)\n",
            "('Statement', 19)\n",
            "('This', 20)\n"
          ]
        }
      ],
      "source": [
        "for i,item in enumerate(vocab.items()):\n",
        "  print(item)\n",
        "  if i >= 20:\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "sK18NH1ykZOJ"
      },
      "outputs": [],
      "source": [
        "class SimpleTokenizerV1:\n",
        "  def __init__(self, vocab):\n",
        "     self.str_to_int = vocab\n",
        "     self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "  def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "    preprocessed = [\n",
        "        item.strip() for item in preprocessed if item.strip()\n",
        "    ]\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "  def decode(self, ids):\n",
        "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TPUCKpbnF33",
        "outputId": "2293202d-ceb6-4cc1-fed5-ebbf6e2e53ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[18, 19, 10, 174, 165, 169, 158, 167, 101, 119, 176, 115, 114, 50, 55, 70, 1, 3, 142, 127, 76, 158, 21, 119, 22, 164, 180, 165, 5, 0, 14, 0, 34, 9, 6, 12, 8, 179, 17, 16, 147, 157, 146, 115, 114, 50, 99, 165, 27, 34, 112, 70, 0, 46, 121, 47, 145, 90, 92, 84, 59, 89, 49, 156, 1, 20, 93, 25, 133, 135, 119, 56, 0, 162, 129, 25, 128, 74, 102, 4, 130, 54, 70, 1, 20, 93, 110, 53, 165, 86, 87, 51, 99, 0, 177, 87, 51, 50, 67, 79, 32, 100, 28, 152, 34, 79, 165, 35, 118, 119, 89, 148, 0, 178, 138, 165, 41, 110, 65, 74, 60, 98, 97, 106, 0, 108, 119, 178, 171, 170, 166, 48, 165, 81, 158, 95, 66, 1, 3, 141, 74, 163, 58, 41, 157, 106, 39, 117, 26, 165, 37, 30, 76, 89, 125, 126, 94, 46, 140, 104, 159, 1, 15, 82, 93, 165, 109, 43, 28, 34, 111, 1, 10, 33, 117, 121, 91, 88, 143, 123, 32, 46, 64, 160, 44, 120, 168, 119, 131, 31, 57, 74, 153, 155, 40, 36, 1, 10, 174, 165, 72, 124, 176, 96, 119, 122, 50, 41, 73, 70, 2, 23, 96, 39, 117, 131, 2, 7, 175, 63, 25, 107, 78, 75, 119, 156, 74, 38, 46, 62, 32, 2, 24, 161, 71, 149, 75, 119, 29, 150, 165, 153, 32, 2, 20, 144, 103, 42, 69, 54, 88, 115, 154, 93, 117, 136, 151, 46, 132, 173, 172, 1, 11, 175, 50, 61, 25, 52, 0, 137, 128, 76, 30, 165, 155, 165, 113, 0, 175, 77, 25, 116, 166, 74, 45, 106, 157, 80, 68, 1, 10, 85, 13, 41, 26, 165, 139, 163, 144, 120, 158, 134, 179, 172, 83, 76, 105, 1]\n"
          ]
        }
      ],
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "ids = tokenizer.encode(raw_text)\n",
        "print(ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "hTt-GD9xnuqg",
        "outputId": "390969d0-b546-424b-aba8-29c7ecfdd8ef"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Research Statement I want to understand the true limits of what neural networks can compute exactly. A recent paper from the University of Waterloo titled “Learning to Add, Multiply, and Execute Algorithmic Instructions Exactly with Neural Networks” showed that shallow neural networks can learn to add and multiply exactly, but only by restructuring inputs into hand crafted input called templates. This is a powerful proof of concept, therefore paves a path for making AI perform computation exactly. This is much closer to how human children learn, where human children can easily generalize algorithms like addition subtraction and generalize to any number of input size, which proves to be much difficult for current large language models, most of which usually use tool call to get the job done. A reason for this could be that models are not able to approximate algorithm from input output pairs itself but rather memorize them. My goal is to move beyond addition and multiplication. I am not only interested in reimplementing other algorithms but determine theoretical bounds on type of possible algorithmic constructions for such template based approach. I want to explore out what kinds of operations can be expressed exactly? What kinds are not possible? Can we design a more general form of templates for arbitrary but defined algorithms? Whether there exist some form of algebraic structures to such algorithms? This research matters because exact computation in neural systems is not properly studied but potentially very valuable. If we can define a clear, provable path from algorithm to template to network, we gain a new tool for building models that generalizes efficiently. I hope I’ll be able to pursue this research on the program with valuable guidance from mentors.'"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "m4Y5zlWioYDM",
        "outputId": "3d5a2b9d-91dd-49aa-b678-97c8732fc7ff"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'might'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3906014420.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"This might not be in vocab, so will generate error\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1290172229.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     ]\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'might'"
          ]
        }
      ],
      "source": [
        "text = \"This might not be in vocab, so will generate error\"\n",
        "tokenizer.encode(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ovs2JodprA1"
      },
      "source": [
        "**Special Context Tokens**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cbb8TOdnpsba",
        "outputId": "191447fa-d138-4cee-9755-8993c06bba61"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "183"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "\n",
        "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
        "len(vocab.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAValDkWuahd",
        "outputId": "1b647e3f-8678-4291-d606-508adc02aebe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('which', 178)\n",
            "('with', 179)\n",
            "('“Learning', 180)\n",
            "('<|endoftext|>', 181)\n",
            "('<|unk|>', 182)\n"
          ]
        }
      ],
      "source": [
        "for i,item in enumerate(list(vocab.items())[-5:]):\n",
        "  print(item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "2Q3KQfOGuuua"
      },
      "outputs": [],
      "source": [
        "class SimpleTokenizerV2:\n",
        "  def __init__(self, vocab):\n",
        "     self.str_to_int = vocab\n",
        "     self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "  def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "    preprocessed = [\n",
        "        item if item in self.str_to_int\n",
        "        else \"<|unk|>\" for item in preprocessed\n",
        "    ]\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "  def decode(self, ids):\n",
        "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "up_Id3IJwle4",
        "outputId": "6f668c5a-0dcf-41de-cef9-f7f0e8dd58eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This might not be in vocab, so will generate error <|endoftext|> Let's check\n"
          ]
        }
      ],
      "source": [
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "\n",
        "text1 = \"This might not be in vocab, so will generate error\"\n",
        "text2 = \"Let's check\"\n",
        "\n",
        "text = \" <|endoftext|> \".join((text1,text2))\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPuA8T1bxgjG",
        "outputId": "91b65146-c31d-4696-fed0-ad2a2e3dad8f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[20, 182, 117, 41, 88, 182, 0, 182, 182, 182, 182, 181, 182, 182, 182, 182]"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.encode(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6s8yLVTaz01Z",
        "outputId": "08adcf77-32ad-4082-c913-cc0c23f3a9a8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'This <|unk|> not be in <|unk|>, <|unk|> <|unk|> <|unk|> <|unk|> <|endoftext|> <|unk|> <|unk|> <|unk|> <|unk|>'"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(tokenizer.encode(text))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
